{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codeTrans_commit_generation_base",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agemagician/CodeTrans/blob/main/prediction/single%20task/commit%20generation/base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9eStCoLX0pZ"
      },
      "source": [
        "**<h3>Generate the commit for github code changes using codeTrans single task training model</h3>**\n",
        "<h4>You can make free prediction online through this \n",
        "<a href=\"https://huggingface.co/SEBIS/code_trans_t5_base_commit_generation\">Link</a></h4> (When using the prediction online, you need to parse and tokenize the code first.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YPrvwDIHdBe"
      },
      "source": [
        "**1. Load necessry libraries including huggingface transformers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FAVWAN1UOJ4"
      },
      "source": [
        "!pip install -q transformers sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53TAO7mmUOyI"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq9v-guFWXHy"
      },
      "source": [
        "**2. Load the token classification pipeline and load it into the GPU if avilabile**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ybX8hZ3UcK2",
        "outputId": "7bcbd02e-a2b5-4008-87d8-e77b018e1127"
      },
      "source": [
        "pipeline = SummarizationPipeline(\n",
        "    model=AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_base_commit_generation\"),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_commit_generation\"),\n",
        "    device=0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkynwKIcEvHh"
      },
      "source": [
        "**3 Give the code for summarization, parse and tokenize it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nld-UUmII-2e"
      },
      "source": [
        "code = \"new file mode 100644\\n index 000000000 . . 892fda21b\\n Binary files/dev/null and b/src/plugins/gateway/lib/joscar.jar differ\\n\" #@param {type:\"raw\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqACvTcjtwYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6f11c0-5b9f-4660-9ca6-7bdb24bd56d2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tokenized_list = WordPunctTokenizer().tokenize(code)\n",
        "tokenized_code = ' '.join(tokenized_list)\n",
        "print(\"tokenized code: \" + tokenized_code)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokenized code: new file mode 100644 index 000000000 . . 892fda21b Binary files / dev / null and b / src / plugins / gateway / lib / joscar . jar differ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVBz9jHNW1PI"
      },
      "source": [
        "**4. Make Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAItQ9U9UwqW",
        "outputId": "b03f03ba-0ea3-4a28-a3ee-5ae379a5b26a"
      },
      "source": [
        "pipeline([tokenized_code])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your max_length is set to 512, but you input_length is only 44. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'Added missing jar file'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    }
  ]
}