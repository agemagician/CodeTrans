{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codeTrans_source_code_summarization_python_base_multitask",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agemagician/CodeTrans/blob/main/prediction/multitask/pre-training/source%20code%20summarization/python/base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9eStCoLX0pZ"
      },
      "source": [
        "**<h3>Summarize the python source code using codeTrans multitask training model</h3>**\n",
        "<h4>You can make free prediction online through this \n",
        "<a href=\"https://huggingface.co/SEBIS/code_trans_t5_base_source_code_summarization_python_multitask\">Link</a></h4> (When using the prediction online, you need to parse and tokenize the code first.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YPrvwDIHdBe"
      },
      "source": [
        "**1. Load necessry libraries including huggingface transformers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FAVWAN1UOJ4"
      },
      "source": [
        "!pip install -q transformers sentencepiece"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53TAO7mmUOyI"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead, SummarizationPipeline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq9v-guFWXHy"
      },
      "source": [
        "**2. Load the token classification pipeline and load it into the GPU if avilabile**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ybX8hZ3UcK2",
        "outputId": "e845815e-5cd7-4a41-c21c-6873eef0e9b3"
      },
      "source": [
        "pipeline = SummarizationPipeline(\n",
        "    model=AutoModelWithLMHead.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_multitask\"),\n",
        "    tokenizer=AutoTokenizer.from_pretrained(\"SEBIS/code_trans_t5_base_source_code_summarization_python_multitask\", skip_special_tokens=True),\n",
        "    device=0\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:852: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkynwKIcEvHh"
      },
      "source": [
        "**3 Give the code for summarization, parse and tokenize it**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nld-UUmII-2e"
      },
      "source": [
        "code = '''with open(\"file.txt\", \"r\") as in_file:\\n    buf = in_file.readlines()\\n\\nwith open(\"file.txt\", \"w\") as out_file:\\n    for line in buf:\\n        if line == \"; Include this text\\n\":\\n            line = line + \"Include below\\n\"\\n        out_file.write(line)''' #@param {type:\"raw\"}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJLeTZ0JtsB5"
      },
      "source": [
        "import tokenize\n",
        "import io\n",
        "\n",
        "def pythonTokenizer(line):\n",
        "    result= []\n",
        "    line = io.StringIO(line) \n",
        "    \n",
        "    for toktype, tok, start, end, line in tokenize.generate_tokens(line.readline):\n",
        "        if (not toktype == tokenize.COMMENT):\n",
        "            if toktype == tokenize.STRING:\n",
        "                result.append(\"CODE_STRING\")\n",
        "            elif toktype == tokenize.NUMBER:\n",
        "                result.append(\"CODE_INTEGER\")\n",
        "            elif (not tok==\"\\n\") and (not tok==\"    \"):\n",
        "                result.append(str(tok))\n",
        "    return ' '.join(result)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqACvTcjtwYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e57e443-9e28-493e-bc12-be1fdb5a33d7"
      },
      "source": [
        "tokenized_code = pythonTokenizer(code)\n",
        "print(\"code after tokenization \" + tokenized_code)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code after tokenization with open ( CODE_STRING , CODE_STRING ) as in_file : buf = in_file . readlines ( )  with open ( CODE_STRING , CODE_STRING ) as out_file : for line in buf :          if line ==   \" ; Include this text   \" :              line = line +   \" Include below  \"          out_file . write ( line )   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVBz9jHNW1PI"
      },
      "source": [
        "**4. Make Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAItQ9U9UwqW",
        "outputId": "4e307dce-565c-42ba-b110-ea296dd696fa"
      },
      "source": [
        "pipeline([tokenized_code])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your max_length is set to 512, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'How to write a text file to a text file in python ?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}